D = {'func': 'D_paper', 'fmap_base': 2048, 'fmap_decay': 1.0, 'fmap_max': 512, 'mbstat_func': 'Tstdeps', 'mbstat_avg': 'all', 'mbdisc_kernels': None, 'use_wscale': True, 'use_gdrop': False, 'use_layernorm': False}
G = {'func': 'G_paper', 'fmap_base': 2048, 'fmap_decay': 1.0, 'fmap_max': 512, 'latent_size': 512, 'normalize_latents': True, 'use_wscale': True, 'use_pixelnorm': True, 'use_leakyrelu': True, 'use_batchnorm': False, 'tanh_at_end': None}
data_dir = datasets
dataset = {'h5_path': 'celeba-128x128_channel_last.h5', 'resolution': 128, 'max_labels': 0, 'mirror_augment': True}
loss = {'type': 'iwass', 'iwass_lambda': 10.0, 'iwass_epsilon': 0.001, 'iwass_target': 1.0, 'cond_type': 'acgan', 'cond_weight': 1.0}
random_seed = 1000
result_dir = results
run_desc = celeba
train = {'separate_funcs': True, 'D_training_repeats': 1, 'G_learning_rate_max': 0.001, 'D_learning_rate_max': 0.001, 'G_smoothing': 0.999, 'adam_beta1': 0.0, 'adam_beta2': 0.99, 'adam_epsilon': 1e-08, 'minibatch_default': 16, 'minibatch_overrides': {}, 'rampup_kimg': 0, 'rampdown_kimg': 0, 'lod_initial_resolution': 4, 'lod_training_kimg': 800, 'lod_transition_kimg': 800, 'total_kimg': 10000, 'gdrop_coef': 0.0}
